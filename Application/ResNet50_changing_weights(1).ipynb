{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet50 changing weights.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq-y1qaqE_gy"
      },
      "source": [
        "Following : https://medium.com/@kenneth.ca95/a-guide-to-transfer-learning-with-keras-using-resnet50-a81a4a28084b\n",
        "Training ResNet50 with freezing all layers except for the last block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwA4ddetE-Nh"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing import image, image_dataset_from_directory\n",
        "from tensorflow.keras.applications import imagenet_utils\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow # cv2.imshow does not work on Google Colab notebooks, --> use cv2_imshow instead"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOXvca0pFYdA"
      },
      "source": [
        "# loading settings\n",
        "model_string = \"resnet50\"  # inceptionv3, resnet50, vgg16\n",
        "dataset_string = \"imagenet\"  # imagenet\n",
        "\n",
        "# other settings\n",
        "heatmap_intensity = 0.5\n",
        "#path_to_dataset = '/content/gdrive/My Drive/What are CNNs looking at/files/New Masks Dataset/'\n",
        "path_to_dataset = '/content/gdrive/My Drive/Kaggle/files/Face Mask Dataset'\n",
        "path_to_dataset = '/content/gdrive/.shortcut-targets-by-id/1wI0fw-edJDu1RgH2Ldk3n4VBJ4m4EJuv/combined'\n",
        "path_to_dataset= '/content/gdrive/MyDrive/combined.rar (Unzipped Files)/combined'\n",
        "base_learning_rate = 0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFDU-LCmmv7n"
      },
      "source": [
        "def gradCAM(orig, model, model_string, DIM, HM_DIM, last_layer, classes, intensity=0.5, res=250):\n",
        "\n",
        "    if model_string == \"inceptionv3\":\n",
        "        from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
        "\n",
        "    elif model_string == \"resnet50\":\n",
        "        from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "\n",
        "    elif model_string == \"vgg16\":\n",
        "        from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "\n",
        "    else:  # use InceptionV3 and imagenet as default\n",
        "        from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
        "\n",
        "\n",
        "    img = image.load_img(orig, target_size=(DIM, DIM))\n",
        "\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "\n",
        "    preds = model.predict(x)\n",
        "    for pred_it, label_it in zip(preds[0], classes):\n",
        "      print(label_it, \": \", pred_it)\n",
        "    print(\"preds\")\n",
        "    print(preds)\n",
        "    prob = np.max(preds[0])\n",
        "    index = list(preds[0]).index(prob)\n",
        "    label = classes[index]\n",
        "    label = \"{}: {:.2f}%\".format(label, prob * 100)\n",
        "    print(\"[INFO] {}\".format(label))\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        last_conv_layer = model.get_layer(last_layer)\n",
        "        iterate = tf.keras.models.Model([model.inputs], [model.output, last_conv_layer.output]) # run model and achive certain output 'last_conv_layer.output'\n",
        "        model_out, last_conv_layer = iterate(x)\n",
        "        class_out = model_out[:, np.argmax(model_out[0])]\n",
        "        grads = tape.gradient(class_out, last_conv_layer) # class out: take derivative; last_conv_layer: variable to derive from\n",
        "        pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    heatmap = tf.reduce_mean(tf.multiply(pooled_grads, last_conv_layer), axis=-1)\n",
        "    heatmap = np.maximum(heatmap, 0)\n",
        "    heatmap /= np.max(heatmap)\n",
        "    heatmap = heatmap.reshape((HM_DIM, HM_DIM))\n",
        "\n",
        "    img_original = cv2.imread(orig)\n",
        "    heatmap = cv2.resize(heatmap, (img_original.shape[1], img_original.shape[0]))\n",
        "    # plt.matshow(heatmap)\n",
        "    # plt.show()\n",
        "\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
        "    # plt.matshow(heatmap)\n",
        "    # plt.show()\n",
        "    img_heatmap = heatmap * intensity + img_original\n",
        "\n",
        "    cv2_imshow(img_heatmap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPfYNA_UFdFu"
      },
      "source": [
        "# model dependent imports, weight loading and model dependent parameter setting\n",
        "K.clear_session()\n",
        "if model_string == \"inceptionv3\":\n",
        "    from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "    from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
        "    Model = InceptionV3\n",
        "    if dataset_string == \"imagenet\":\n",
        "        model = InceptionV3(weights='imagenet')\n",
        "    target_input_dimension = 299\n",
        "    heatmap_dimension = 8\n",
        "    last_layer_name = 'conv2d_93'\n",
        "elif model_string == \"resnet50\":\n",
        "    from tensorflow.keras.applications import ResNet50\n",
        "    from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "    Model = ResNet50\n",
        "    if dataset_string == \"imagenet\":\n",
        "        model = ResNet50(weights=\"imagenet\")\n",
        "    target_input_dimension = 224\n",
        "    heatmap_dimension = 7\n",
        "    last_layer_name = 'conv5_block3_3_conv'\n",
        "elif model_string == \"vgg16\":\n",
        "    from tensorflow.keras.applications import VGG16\n",
        "    from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "    Model = VGG16\n",
        "    if dataset_string == \"imagenet\":\n",
        "        model = VGG16(weights=\"imagenet\")\n",
        "    target_input_dimension = 224\n",
        "    heatmap_dimension = 14\n",
        "    last_layer_name = 'block5_conv3'\n",
        "else:  # use InceptionV3 and imagenet as default\n",
        "    from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "    from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
        "    if dataset_string == \"imagenet\":\n",
        "        model = InceptionV3(weights='imagenet')\n",
        "    target_input_dimension = 299\n",
        "    heatmap_dimension = 8\n",
        "    last_layer_name = 'conv2d_93'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqEOtmKaFgve"
      },
      "source": [
        "# mount google drive to access database\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DRNlWB6Fi9y"
      },
      "source": [
        "#%cd /content/gdrive/.shortcut-targets-by-id/1D594PlXLC7S5YLnKD2s09vbezmGes8H6/What are CNNs looking at\n",
        "#%cd /content/gdrive/Computers/My Laptop/combined\n",
        "#%cd /content/gdrive/.shortcut-targets-by-id/1wI0fw-edJDu1RgH2Ldk3n4VBJ4m4EJuv/combined\n",
        "#!ls -la gdrive/MyDrive/\n",
        "%cd /content/gdrive/MyDrive/combined.rar (Unzipped Files)/combined\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbnII4EAFsGA"
      },
      "source": [
        "# import database\n",
        "\n",
        "train_dir = os.path.join(path_to_dataset, 'Train')\n",
        "validation_dir = os.path.join(path_to_dataset, 'Validation')\n",
        "test_dir = os.path.join(path_to_dataset, 'Test')\n",
        "BATCH_SIZE = 64\n",
        "IMG_SIZE = (224, 224)\n",
        "\n",
        "train_dataset = image_dataset_from_directory(train_dir,\n",
        "                                             shuffle=True,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             image_size=IMG_SIZE)\n",
        "validation_dataset = image_dataset_from_directory(validation_dir,\n",
        "                                                  shuffle=True,\n",
        "                                                  batch_size=BATCH_SIZE,\n",
        "                                                  image_size=IMG_SIZE)\n",
        "test_dataset = image_dataset_from_directory(test_dir,\n",
        "                                             shuffle=True,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             image_size=IMG_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lakgv3X2Fs9O"
      },
      "source": [
        "class_names = train_dataset.class_names\n",
        "print(class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxKydcavFxBd"
      },
      "source": [
        "# training the last block of Resnet50 since we are using a different dataset wih imagenet weights\n",
        "# creating the model\n",
        "\n",
        "# currently ResNet50 is hard coded\n",
        "K.clear_session()\n",
        "\n",
        "# load ResNet50 without dense layers\n",
        "base_model = ResNet50(input_shape=(target_input_dimension, \n",
        "                               target_input_dimension, \n",
        "                               3), \n",
        "                  include_top=False, \n",
        "                  weights='imagenet')\n",
        "for layer in base_model.layers[:81]:\n",
        "  layer.trainable = False\n",
        "# printing the layers that are trainable\n",
        "for i, layer in enumerate(base_model.layers):\n",
        "  print(i, layer.name, \"-\", layer.trainable)\n",
        "\n",
        "inputs = tf.keras.Input(shape=(target_input_dimension, \n",
        "                               target_input_dimension, \n",
        "                               3))\n",
        "x = preprocess_input(inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5OhyuMRFx2Y"
      },
      "source": [
        "to_res = (224,224)\n",
        "model = tf.keras.models.Sequential()\n",
        "#model.add(tf.keras.layers.Lambda(lambda image: tf.image.resize(image, to_res))) \n",
        "inputs = tf.keras.Input(shape=(target_input_dimension, \n",
        "                               target_input_dimension, \n",
        "                               3), name=\"model_input\")\n",
        "x = preprocess_input(inputs)\n",
        "x = base_model(x)  # Why training=False?\n",
        "x = tf.identity(x) # needed to be able to obtain heatmap\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "x = tf.keras.layers.Dense(2, activation='softmax')(x)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=x)\n",
        "  \n",
        "\n",
        "#check_point = tf.keras.callbacks.ModelCheckpoint(filepath=\"cifar10.h5\",\n",
        " #                                         monitor=\"val_acc\",\n",
        "  #                                        mode=\"max\",\n",
        "   #                                       save_best_only=True,\n",
        "    #                                     )\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "#history = model.fit(x_train, y_train, batch_size=32, epochs=10, verbose=1,\n",
        "#                    validation_data=(x_test, y_test),\n",
        "#                    callbacks=[check_point])\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=10,\n",
        "                    validation_data=validation_dataset)\n",
        "model.summary()\n",
        "# Plot training and validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "%cd /content/gdrive/.shortcut-targets-by-id/1D594PlXLC7S5YLnKD2s09vbezmGes8H6/What are CNNs looking at\n",
        "!mkdir -p saved_model\n",
        "model.save('saved_model/new_resnet50trained_wrongmask')\n",
        "\n",
        "# heatmaps\n",
        "%cd /content/gdrive/.shortcut-targets-by-id/1wI0fw-edJDu1RgH2Ldk3n4VBJ4m4EJuv/combined\n",
        "images = ['Test/Mask/45.png','Test/Mask/2086.jpg','Test/Mask/2267.png','Test/Mask/2190.png', 'Test/Non Mask/45.png', 'Test/Non Mask/5878.png','Test/Non Mask/real_01047.jpg', 'Test/Non Mask/real_01072.jpg']\n",
        "\n",
        "DIM = 224\n",
        "for orig in images:\n",
        "  gradCAM(orig, model, model_string, target_input_dimension, heatmap_dimension,'tf.identity',class_names)\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "%cd /content/gdrive/.shortcut-targets-by-id/1D594PlXLC7S5YLnKD2s09vbezmGes8H6/What are CNNs looking at\n",
        "images = ['maskWrong.PNG','nomask.PNG','abin_halfmask.jpg', 'maskdown.PNG', 'maskdown2.PNG','maskdown3.PNG','maskchin.PNG','maskchin1.PNG']\n",
        "\n",
        "\n",
        "DIM = 224\n",
        "for orig in images:\n",
        "  gradCAM(orig, model, model_string, target_input_dimension, heatmap_dimension,'tf.identity',class_names)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE1-9pgfr_Vc"
      },
      "source": [
        "history2 = model.fit(train_dataset,\n",
        "                    epochs=6,\n",
        "                    validation_data=validation_dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEJAlRFEo8st"
      },
      "source": [
        "#%cd /content/gdrive/.shortcut-targets-by-id/1D594PlXLC7S5YLnKD2s09vbezmGes8H6/What are CNNs looking at\n",
        "images = ['maskWrong.PNG','nomask.PNG','abin_halfmask.jpg', 'maskdown.PNG', 'maskdown2.PNG','maskdown3.PNG','maskchin.PNG','maskchin1.PNG']\n",
        "\n",
        "DIM = 224\n",
        "for orig in images:\n",
        "  gradCAM(orig, model, model_string, target_input_dimension, heatmap_dimension,'tf.identity_1',class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYqFzaxCoSfC"
      },
      "source": [
        "# Plot training and validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y4lpDIQttLU"
      },
      "source": [
        "history2 = model.fit(train_dataset,\n",
        "                    epochs=2,\n",
        "                    validation_data=validation_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4W2HCtPwbs-"
      },
      "source": [
        "history2 = model.fit(train_dataset,\n",
        "                    epochs=4,\n",
        "                    validation_data=validation_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHikt_Kz1Cw1"
      },
      "source": [
        "orig ='maskWrong.PNG'\n",
        "#orig = 'files/12k Face Mask Dataset/Test/WithoutMask/1439.png'\n",
        "DIM = 224\n",
        "gradCAM(orig, model, model_string, target_input_dimension, heatmap_dimension,'tf.identity',class_names)\n",
        "orig ='nomask.PNG'\n",
        "#orig = 'files/12k Face Mask Dataset/Test/WithoutMask/1439.png'\n",
        "DIM = 224\n",
        "gradCAM(orig, model, model_string, target_input_dimension, heatmap_dimension,'tf.identity',class_names)\n",
        "orig ='abin_halfmask.jpg'\n",
        "#orig = 'files/12k Face Mask Dataset/Test/WithoutMask/1439.png'\n",
        "DIM = 224\n",
        "gradCAM(orig, model, model_string, target_input_dimension, heatmap_dimension,'tf.identity',class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQhMmGGMvzNJ"
      },
      "source": [
        "orig ='maskchin.PNG'\n",
        "#orig = 'files/12k Face Mask Dataset/Test/WithoutMask/1439.png'\n",
        "DIM = 224\n",
        "gradCAM(orig, model, model_string, target_input_dimension, heatmap_dimension,'tf.identity',class_names)\n",
        "orig ='abin_nomask.jpg'\n",
        "#orig = 'files/12k Face Mask Dataset/Test/WithoutMask/1439.png'\n",
        "DIM = 224\n",
        "gradCAM(orig, model, model_string, target_input_dimension, heatmap_dimension,'tf.identity',class_names)\n",
        "orig ='SBhand.jpg'\n",
        "#orig = 'files/12k Face Mask Dataset/Test/WithoutMask/1439.png'\n",
        "DIM = 224\n",
        "gradCAM(orig, model, model_string, target_input_dimension, heatmap_dimension,'tf.identity',class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0LpprmC6Mol"
      },
      "source": [
        "%cd /content/gdrive/.shortcut-targets-by-id/1D594PlXLC7S5YLnKD2s09vbezmGes8H6/What are CNNs looking at\n",
        "orig ='maskchin1.PNG'\n",
        "#orig = 'files/12k Face Mask Dataset/Test/WithoutMask/1439.png'\n",
        "DIM = 224\n",
        "gradCAM(orig, model, model_string, target_input_dimension, heatmap_dimension,'tf.identity',class_names)\n",
        "orig ='maskdown.PNG'\n",
        "#orig = 'files/12k Face Mask Dataset/Test/WithoutMask/1439.png'\n",
        "DIM = 224\n",
        "gradCAM(orig, model, model_string, target_input_dimension, heatmap_dimension,'tf.identity',class_names)\n",
        "orig ='maskdown2.PNG'\n",
        "#orig = 'files/12k Face Mask Dataset/Test/WithoutMask/1439.png'\n",
        "DIM = 224\n",
        "gradCAM(orig, model, model_string, target_input_dimension, heatmap_dimension,'tf.identity',class_names)\n",
        "orig ='maskdown3.PNG'\n",
        "#orig = 'files/12k Face Mask Dataset/Test/WithoutMask/1439.png'\n",
        "DIM = 224\n",
        "gradCAM(orig, model, model_string, target_input_dimension, heatmap_dimension,'tf.identity',class_names)\n",
        "orig =' JLM_3a.jpg'\n",
        "#orig = 'files/12k Face Mask Dataset/Test/WithoutMask/1439.png'\n",
        "DIM = 224\n",
        "gradCAM(orig, model, model_string, target_input_dimension, heatmap_dimension,'tf.identity',class_names)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}